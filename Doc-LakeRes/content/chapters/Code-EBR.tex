%==============================================================================
% CHAPITRE DU CODE EBR
%==============================================================================

\chapter{Code - EBR}
\label{chap:code_ebr}

% Mini-table des matières du chapitre
\minitableofcontents

\newpage

%------------------------------------------------------------------------------
% CONTENU
%------------------------------------------------------------------------------

\section{App EBR commun.py}
\label{sec:app_ebr_commun}

\subsection{Chargements des bibliothèques, modules et du dossier racines}
\label{sec:chargements_bibliotheques_modules_dossier_racines}

Cette section permet l'importation de l'ensemble des librairies utilisées par le code, dont celles de Python, celles de librairies externes et les codes d'HydroModPy fonctionnant en POO (programmation orientée objet). Ces différentes librairies sont toutes incluses dans l'environnement \inlinecode{Hydromodpy-0.1} préalablement installé.


En amont de ces librairies, une section \inlinecode{# Filtrer les avertissements} est à renseigner à chaque début de code afin que les alertes de \inlinecode{DeprecationWarnings} ne s'affichent pas, voir \ref{sec:deprecationwarnings}.

\subsection{LogManager}
\label{sec:logmanager-ebr}

La \inlinecode{class LogManager} permet de gérer l'interface verbale entre l'utilisateur et le code, en faisant remonter des logs selon différentes classes avec plus ou moins de précisions et de messages selon le mode choisi. Pour paramétrer le \inlinecode{LogManager}, voir la section \ref{sec:logmanager}.

\section{Initialisation de la classe climatiques}
\label{sec:initialisation_climatiques}

\subsection{Réanalyse Surfex}
\label{sec:reanalyse_surfex}

\subsection{Méthode de création d'un csv pour données climatiques}
\label{sec:csv_donnees_climatiques}

En temps normal, HydroModPy (à l’échelle de la France) fonctionne automatiquement avec les données SIM2. Pour la Bretagne, la recharge et le runoff sont modifiés à partir des données de réanalyse. Ici, des données ISBA brutes issues du serveur FTP de Météo-France sont utilisées directement.

Ce procédé nécessite de fusionner des fichiers NetCDF à chaque itération, ce qui est coûteux en calcul. De plus, les données de réanalyses doivent être extraites dans chaque dossier de sortie, sauf si elles sont externalisées au préalable.

Une méthode plus simple consiste à exécuter une dernière fois la méthode classique, puis à créer un \inlinecode{DataFrame} pour exporter l’ensemble des données climatiques, comme ci-dessous :

\begin{minted}{python}
#============================================================================= 
# Exportation des données climatiques 
# ============================================================================= 
# df_climatic = pd.DataFrame({
#     'recharge': BV.climatic.recharge,
#     'runoff': BV.climatic.runoff,
#     'precip': BV.climatic.precip,
#     'evt': BV.climatic.evt,
#     'etp': BV.climatic.etp,
#     't': BV.climatic.t,
# })
# df_climatic.to_csv(os.path.join(data_path, 'Meteo', 'Historiques SIM2', 'climatic_data.csv'))
\end{minted}

Ensuite, toute la classe climatique peut etre mise en commentaire afin de ne garder que la lecture du CSV précédemment créé, comme ci-dessous :

\begin{minted}{python}
df_climatic = pd.read_csv(
    os.path.join(data_path, 'Meteo', 'Historiques SIM2', 'climatic_data.csv'),
    index_col=0, parse_dates=True
)
df_climatic.index = pd.to_datetime(df_climatic.index)
df_climatic = df_climatic.loc[
    (df_climatic.index >= pd.Timestamp("01/01/{}".format(first_year))) &
    (df_climatic.index <= pd.Timestamp("31/12/{}".format(last_year)))
]

agg_dict = {
    'recharge': 'sum',
    'runoff': 'sum',
    'precip': 'sum',
    'evt': 'sum',
    'etp': 'sum',
    't': 'mean'
}
df_climatic = df_climatic.resample(freq_input).agg(agg_dict)

BV.climatic.recharge = df_climatic['recharge']
BV.climatic.runoff = df_climatic['runoff']
BV.climatic.precip = df_climatic['precip']
BV.climatic.evt = df_climatic['evt']
BV.climatic.etp = df_climatic['etp']
BV.climatic.t = df_climatic['t']

first_clim = BV.climatic.recharge[0]
BV.climatic.update_first_clim(first_clim)
\end{minted}

\begin{TipBox}
    \textbf{Remarque :} Il est conseillé d’exporter le fichier en données journalières, puis de procéder à la réanalyse (hebdomadaire, mensuelle, etc.) lors de l’import. La sélection automatique des dates minimale et maximale peut être réalisée à l’aide des arguments déjà renseignés.
\end{TipBox}

\section{Paramétrisation}
\label{sec:parametrisation}

\subsection{Simplex de Nelder-Mead}
\label{sec:simplex}

Le Simplex de Nelder-Mead est un algorithme d'optimisation non-linéaire adapté aux problèmes où le calcul des dérivées est complexe. Son principe repose sur la manipulation d'une figure géométrique à \(N+1\) sommets dans un espace à \(N\) dimensions \parencite{nelderSimplexMethodFunction1965,lagariasConvergencePropertiesNelderMead1998}.

\vspace{1em}

\subsubsection{Principe et enchaînement des opérations}

L'algorithme utilise quatre opérations géométriques principales qui s'enchaînent selon un arbre de décision précis. À chaque itération, les valeurs de la fonction objectif aux sommets sont d'abord ordonnées:
\begin{equation}\label{eq:ordre_simplex}
f(x_1) \leq f(x_2) \leq \ldots \leq f(x_{N+1})
\end{equation}

Où $x_1$ est le meilleur sommet (valeur de fonction la plus basse) et $x_{N+1}$ le pire sommet (valeur de fonction la plus élevée). Le centroïde des $N$ meilleurs sommets est calculé comme $x_0 = \frac{1}{N}\sum_{i=1}^{N}x_i$.

\vspace{1em}

\subsubsection{Enchaînement des opérations dans une itération du Simplex de Nelder-Mead:}

\begin{enumerate}
    \item \textbf{Réflexion} (toujours effectuée en premier) :
    
       Cette étape vise à explorer l'espace des solutions en reflétant le pire point du simplex (le sommet avec la plus mauvaise valeur de la fonction objectif) à travers le centre de gravité des autres points. Cela permet souvent de s'approcher d'une zone de meilleure performance.
    
       \begin{itemize}
       \item Calculer le point réfléchi :
       \begin{equation}
       x_r = x_0 + \alpha (x_0 - x_{N+1})
       \end{equation}
       où $x_0$ est le centroïde des meilleurs sommets, $x_{N+1}$ est le pire point, et $\alpha$ est le coefficient de réflexion (généralement égal à 1). Ce coefficient détermine la distance à laquelle le point est réfléchi au-delà du centroïde, le nouveau point $x_r$ est donc situé à une distance proportionnelle à la distance entre le centroïde et le pire point.
       
       On évalue ensuite la fonction objectif au point réfléchi :
       \begin{equation}
       f_r = f(x_r)
       \end{equation}
       Cette valeur déterminera l'étape suivante de l'algorithme.
       \end{itemize}
    
\vspace{1em}
    
    \item \textbf{Décision} (une seule branche est suivie en fonction de la qualité du point réfléchi) :
    
       Selon la valeur de $f_r$, plusieurs scénarios sont envisagés pour ajuster le simplex et continuer l'optimisation. Ce processus de décision permet à l'algorithme de s'adapter à la topologie locale de la fonction objectif.
    
       \begin{itemize}

\vspace{1em}

       \item \textbf{Cas 1 : Acceptation simple} \\
       Si le point réfléchi est meilleur qu'une majorité des points mais pas le meilleur :
       \begin{equation}
       f(x_1) \leq f_r < f(x_N)
       \end{equation}
       alors on remplace simplement le pire point par le point réfléchi :
       \begin{equation}
       x_{N+1} \leftarrow x_r
       \end{equation}
       Cette situation correspond à un progrès modéré dans la recherche de l'optimum, sans nécessiter d'exploration supplémentaire dans cette direction.

       \begin{InfoBox}
            Cependant, lorsque ce même point deviendra le moins bon, l'exploration pourra reprendre dans cette direction.
       \end{InfoBox}

\vspace{1em}
    
       \item \textbf{Cas 2 : Expansion} \\
       Si le point réfléchi est meilleur que le meilleur point actuel :
       \begin{equation}
       f_r < f(x_1)
       \end{equation}
       on tente d'exploiter cette direction prometteuse en calculant un point encore plus éloigné. L'expansion permet de progresser plus rapidement vers l'optimum lorsqu'une direction favorable est identifiée :
       \begin{equation}
       x_e = x_0 + \beta (x_r - x_0)
       \end{equation}
       où $\beta$ est le coefficient d'expansion (supérieur à 1, égal à 2 dans le cas de l'utilisation de la méthode \inlinecode{minimize} dans \filepath{SciPy}). Ce coefficient détermine jusqu'où on étend la recherche dans la direction prometteuse.
       
\vspace{1em}
    
       \item \textbf{Cas 3 : Contraction externe} \\
       Si le point réfléchi est moins bon que la plupart des points mais meilleur que le pire :
       \begin{equation}
       f(x_N) \leq f_r < f(x_{N+1})
       \end{equation}
       on essaie un compromis plus modéré en effectuant une contraction externe. Cette opération permet d'explorer l'espace entre le centroïde et le point réfléchi :
       \begin{equation}
       x_c = x_0 + \gamma (x_r - x_0)
       \end{equation}
       où $\gamma$ est le coefficient de contraction. Ce coefficient restreint l'exploration à une zone plus proche du centroïde.
       
       On évalue ensuite :
       \begin{equation}
       f_c = f(x_c)
       \end{equation}
       - Si $f_c \leq f_r$ : on accepte $x_c$ (la contraction a trouvé un point meilleur)
       - Sinon : on procède à un rétrécissement du simplex (la contraction n'a pas été efficace)

\vspace{1em}
    
       \item \textbf{Cas 4 : Contraction interne} \\
       Si le point réfléchi est encore pire que le pire actuel :
       \begin{equation}
       f_r \geq f(x_{N+1})
       \end{equation}
       on tente une contraction plus prudente en explorant l'espace entre le centroïde et le pire point. Cette stratégie est adoptée lorsque la direction de réflexion s'avère défavorable :
       \begin{equation}
       x_c = x_0 + \gamma (x_{N+1} - x_0)
       \end{equation}
       où $\gamma$ est à nouveau le coefficient de contraction.
       
       On évalue :
       \begin{equation}
       f_c = f(x_c)
       \end{equation}
       - Si $f_c < f(x_{N+1})$ : on remplace le pire point par $x_c$ (la contraction interne a été bénéfique)
       - Sinon : un rétrécissement complet du simplex devient nécessaire (la topologie locale est complexe et nécessite une restructuration)
       \end{itemize}

\vspace{1em}

    \item \textbf{Rétrécissement} (seulement si la contraction a échoué) :
    
       Cette étape drastique vise à « resserrer » le simplex autour du meilleur point trouvé pour éviter de rester bloqué dans des zones peu prometteuses. Le rétrécissement est une stratégie de dernier recours qui indique souvent que l'algorithme approche d'un minimum local ou rencontre une région difficile de la fonction objectif.
    
       \begin{itemize}
       \item Pour chaque $i = 2, \ldots, N+1$ :
       \begin{equation}
       x_i \leftarrow x_1 + \delta (x_i - x_1)
       \end{equation}
       où $\delta$ est le coefficient de rétrécissement. Ce coefficient détermine à quel point le simplex se contracte autour du meilleur point.
       
       Cette opération réduit la taille du simplex et recentre la recherche autour du meilleur point actuel, permettant une exploration plus fine et locale de l'espace des paramètres.
       \end{itemize}
    \end{enumerate}
    
Où les coefficients standards sont $\alpha = 1$ (réflexion), $\beta = 2$ (expansion), $\gamma = 0.5$ (contraction) et $\delta = 0.5$ (rétrécissement). La méthode \inlinecode{minimize} de \filepath{Scipy} utilise une implémentation adaptative, ils dépendent de la dimension $N$ du problème( basé sur les travaux de \cite{nelderSimplexMethodFunction1965}; \cite{lagariasConvergencePropertiesNelderMead1998}; et \cite{gaoImplementingNelderMeadSimplex2012}):

\begin{align}
\alpha &= 1, \quad \beta = 1 + \frac{2}{N}, \quad \gamma = 0.75 - \frac{0.5}{N}, \quad \delta = 1 - \frac{1}{N}
\end{align}

Cette adaptation permet d'optimiser le comportement de l'algorithme en fonction de la dimensionnalité du problème. Pour les problèmes de grande dimension, les coefficients sont ajustés pour favoriser une exploration plus équilibrée de l'espace des paramètres.

\vspace{1em}

\begin{InfoBox}
    \noindent\textbf{Points importants à noter:}
        \begin{itemize}[leftmargin=1cm]
            \item Une seule des branches de l'arbre de décision est suivie à chaque itération, ce qui rend l'algorithme efficace en termes de nombre d'évaluations de la fonction objectif. Cela signifie que l'algorithme choisit toujours le meilleur mouvement possible à chaque étape.
            
            \item Le rétrécissement n'est appliqué qu'en dernier recours, si les contractions échouent, car il s'agit d'une opération coûteuse nécessitant $N$ évaluations supplémentaires de la fonction. Cette opération réduit la taille du simplex autour du meilleur point pour affiner la recherche localement.
            
            \item Pour les problèmes de grande dimension (>5 paramètres), les opérations de réflexion deviennent dominantes mais moins efficaces. Les mouvements géométriques du simplex ne s'adaptent pas bien à la complexité croissante de l'espace de recherche, ce qui ralentit la convergence.
            
            \item L'algorithme ne nécessite pas le calcul de dérivées, ce qui le rend particulièrement utile pour l'optimisation de fonctions non différentiables, bruitées ou irrégulières. Cette caractéristique est idéale pour les modèles hydrogéologiques où les relations entre paramètres et performance peuvent présenter des discontinuités.
        \end{itemize}
\end{InfoBox}

\subsubsection{Normalisation et mise à l'échelle des paramètres}

Pour garantir une convergence efficace, nous normalisons tous les paramètres dans l'intervalle [0,1] avant optimisation:
\begin{equation}
x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}
\end{equation}

Cette normalisation est particulièrement importante pour la conductivité hydraulique ($K$) qui varie sur plusieurs ordres de grandeur. Pour ce paramètre, nous utilisons une échelle logarithmique:
\begin{equation}
K_{norm} = \frac{\log_{10}(K) - \log_{10}(K_{min})}{\log_{10}(K_{max}) - \log_{10}(K_{min})}
\end{equation}


\subsubsection{Implémentation dans le code:}

L'algorithme est intégré via la bibliothèque SciPy:

\begin{minted}{python}
from scipy.optimize import minimize

# Normalisation des paramètres
def normalize(x, xmin, xmax):
    """Normalise une valeur x selon les bornes xmin et xmax"""
    return (x - xmin) / (xmax - xmin)

def denormalize(x_norm, xmin, xmax):
    """Dénormalise une valeur x_norm selon les bornes xmin et xmax"""
    return x_norm * (xmax - xmin) + xmin

# Fonction objectif normalisée
def erreur_modele_norm(params_norm):
    # Dénormalisation des paramètres
    log_hk_value = denormalize(params_norm[0], log_hk_min, log_hk_max)
    hk_value = 10**log_hk_value
    sy_value = denormalize(params_norm[1], sy_min, sy_max)
    thick_value = denormalize(params_norm[2], thick_min, thick_max)
    
    # Mise à jour du modèle avec les nouveaux paramètres
    BV.hydraulic.update_hk(hk_value)
    BV.hydraulic.update_sy(sy_value)
    BV.hydraulic.update_thick(thick_value)
    
    # Simulation du modèle avec ces paramètres
    model_modflow = BV.preprocessing_modflow()
    success_modflow = BV.processing_modflow(model_modflow)
    BV.postprocessing_timeseries(model_modflow)
    
    # Calcul du critère de Nash-Sutcliffe
    nse = 1 - (numerator / denominator) 
    return 1 - nse  # On minimise 1-NSE

# Exécution de l'optimisation
result = minimize(
    erreur_modele_norm, 
    x0_norm,  # Paramètres initiaux normalisés
    method='Nelder-Mead',
    options={
        'xatol': 0.01,  # Tolérance sur les paramètres
        'fatol': 0.01,  # Tolérance sur la fonction
        'maxiter': 200,  # Nombre max d'itérations
        'disp': True     # Affichage des informations
    }
)
\end{minted}

\subsubsection{Paramètres calibrés}

Dans notre implémentation, trois paramètres hydrogéologiques fondamentaux sont calibrés, la conductivité hydraulique ($K$), la porosité efficace ($S_y$) et l'épaisseur de la couche aquifère ($e$). Ces paramètres sont cruciaux pour simuler le comportement hydraulique du modèle.

\subsubsection{Fonction objectif et sélection des données}

La fonction objectif utilisée dans notre cas est le critère de Nash-Sutcliffe - NSE \parencite{nashRiverFlowForecasting1970}, qui évalue la qualité de la simulation par rapport aux données observées. Le NSE est défini comme suit:

\begin{equation}\label{eq:nse}
    \text{NSE} = 1 - \frac{\sum_{t=1}^{T}(Q_{obs,t} - Q_{sim,t})^2}{\sum_{t=1}^{T}(Q_{obs,t} - \bar{Q}_{obs})^2}
\end{equation}

Notre implémentation permet de sélectionner précisément les données utilisées pour la calibration:

\begin{minted}{python}
def filter_dates(dates):
    """Filtre les dates selon des critères temporels et saisonniers"""
    mask = pd.Series(True, index=dates)
    
    if use_time_filter:
        mask = mask & (dates >= calib_start_date) & (dates <= calib_end_date)
    
        if use_seasonal_filter:
            def is_in_season(date):
                start = pd.Timestamp(date.year, season_start_month, season_start_day)
                if season_end_month < season_start_month:
                    end = pd.Timestamp(date.year + 1, season_end_month, season_end_day)
                else:
                    end = pd.Timestamp(date.year, season_end_month, season_end_day)
                return (date >= start) & (date <= end)
            
            seasonal_mask = dates.map(is_in_season)
            mask = mask & seasonal_mask

    return mask
\end{minted}

Cette approche permet de concentrer la calibration sur des périodes représentatives, en excluant si nécessaire des événements extrêmes ou des saisons particulières, pour optimiser les périodes d'étiage par exemple.

\subsubsection{Avantages et limitations}

Dans notre contexte hydrogéologique, le Simplex offre un excellent compromis entre simplicité d'implémentation et efficacité de calibration pour les principaux paramètres qui contrôlent le comportement hydraulique du modèle.

Cependant, il est important de garder à l'esprit ses limitations, notamment la sensibilité à l'initialisation et la possibilité de convergence vers des minima locaux. 

Pour éviter cela, \textbf{il est recommandé de tester plusieurs initialisations} afin de s'assurer que les différents résultats convergent vers des solutions similaires.